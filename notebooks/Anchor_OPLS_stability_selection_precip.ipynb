{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6271f056",
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "\n",
    "import xarray as xr\n",
    "import netCDF4 as nc\n",
    "import numpy as np\n",
    "\n",
    "import os\n",
    "\n",
    "import datetime as dt\n",
    "import pickle\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import r2_score\n",
    "import random\n",
    "from tqdm import tqdm\n",
    "from collections import Counter\n",
    "from AnchorOptimalProjector import *\n",
    "import scipy\n",
    "import h5py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "91bc34d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "models = ['CanESM5', 'MIROC-ES2L', 'MPI-ESM1-2-LR', 'MIROC6', 'CESM2']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "000cc424",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import h5py\n",
    "import random\n",
    "import numpy as np\n",
    "import xarray as xr\n",
    "\n",
    "def load_model_data(model, var, path='../data/'):\n",
    "    \"\"\"\n",
    "    Load model data from either a NetCDF (.nc) or a HDF5 (.h5) file.\n",
    "    \n",
    "    Args:\n",
    "        model (str): Model name.\n",
    "        var (str): Variable name.\n",
    "        path (str, optional): Path to the data directory. Default is '../data/'.\n",
    "        \n",
    "    Returns:\n",
    "        numpy.ndarray: Array containing the loaded model data.\n",
    "    \"\"\"\n",
    "    # Loading data file\n",
    "    file_path = os.path.join(path, '{}_{}.nc'.format(model, var))\n",
    "    ds = xr.open_dataset(file_path)\n",
    "    # Getting variable data\n",
    "    array = ds[var].values\n",
    "    # Close the dataset\n",
    "    ds.close()\n",
    "    return array\n",
    "\n",
    "def get_data_shape_lat_lon(model='CanESM5', var='pr', path='../data/'):\n",
    "    \"\"\"\n",
    "    Get the shape and latitude/longitude coordinates of the data from a NetCDF file.\n",
    "    \n",
    "    Args:\n",
    "        model (str, optional): Model name. Default is 'CanESM5'.\n",
    "        var (str, optional): Variable name. Default is 'pr'.\n",
    "        path (str, optional): Path to the data directory. Default is '../data/'.\n",
    "        \n",
    "    Returns:\n",
    "        tuple: Shape of the data array, latitude coordinate, and longitude coordinate.\n",
    "    \"\"\"\n",
    "    file_path = os.path.join(path, '{}_{}.nc'.format(model, var))\n",
    "    ds = xr.open_dataset(file_path)\n",
    "    # Getting variable data shape\n",
    "    shape = ds[var].values.shape\n",
    "    # Close the dataset\n",
    "    ds.close()\n",
    "    return shape, ds['lat'], ds['lon']\n",
    "\n",
    "def load_data_models(models, var='pr', n_sample=10, path='../data/', coarse=12, window_size=10, pkl=False):\n",
    "    \"\"\"\n",
    "    Load data from multiple models, preprocess it, and return the processed data for further analysis.\n",
    "    \n",
    "    Args:\n",
    "        models (list): List of model names.\n",
    "        var (str, optional): Variable name. Default is 'pr'.\n",
    "        n_sample (int, optional): Number of samples to select randomly from each model's data. Default is 10.\n",
    "        path (str, optional): Path to the data directory. Default is '../data/'.\n",
    "        coarse (int, optional): Coarsening factor for spatial aggregation. Default is 12.\n",
    "        window_size (int, optional): Size of the moving average window. Default is 10.\n",
    "        pkl (bool, optional): If True, loads data from a .h5 file; otherwise, from a .nc file. Default is False.\n",
    "        \n",
    "    Returns:\n",
    "        tuple: Arrays containing the regional internal variability, input data, and target data.\n",
    "    \"\"\"\n",
    "    X, y, A = None, None, None\n",
    "    for model in models:\n",
    "        # Load model data\n",
    "        array = load_model_data(model, var=var, path=path)\n",
    "        \n",
    "        shape = array.shape\n",
    "\n",
    "        # Generate random values from a normal distribution with mean 0 and standard deviation 0.1\n",
    "        random_values = np.random.normal(loc=0, scale=0.1, size=shape)\n",
    "\n",
    "        # Replace NaN values with random values\n",
    "        array[np.isnan(array)] = random_values[np.isnan(array)]\n",
    "        \n",
    "        # Randomly select n_sample indices\n",
    "        idxs = random.sample(range(shape[0]*shape[1]), n_sample)\n",
    "        \n",
    "        # Reshape data for spatial aggregation\n",
    "        regional = array.reshape(shape[0], shape[1], shape[2]//coarse, coarse, shape[3]//coarse, coarse)\n",
    "        \n",
    "        # Spatially aggregate data\n",
    "        X_temp = regional.mean(axis=(3, 5))\n",
    "        rmt = X_temp.mean(axis=0)\n",
    "        \n",
    "        # Calculate regional internal variability\n",
    "        riv = rmt - X_temp\n",
    "        \n",
    "        # Create a 1D convolution kernel for the moving average\n",
    "        kernel = np.ones(window_size) / window_size\n",
    "        \n",
    "        # Smooth the regional internal variability using a moving average\n",
    "        A_temp = np.apply_along_axis(lambda x: np.convolve(x, kernel, mode='same'), axis=1, arr=riv)\n",
    "        \n",
    "        # Reshape data for further processing\n",
    "        X_temp = X_temp.reshape(shape[0]*shape[1], shape[2]*shape[3]//(coarse**2))\n",
    "        y_temp = np.tile(array.mean(axis=0), (shape[0], 1, 1)).reshape(shape[0]*shape[1], shape[2]*shape[3])\n",
    "        A_temp = A_temp.reshape(shape[0]*shape[1], shape[2]*shape[3]//(coarse**2))\n",
    "        # Concatenate data from current model with existing data\n",
    "        if X is None:\n",
    "            X = X_temp[idxs,:]\n",
    "            y = y_temp[idxs,:]\n",
    "            A = A_temp[idxs, :] \n",
    "        else:\n",
    "            X = np.vstack((X, X_temp[idxs,:]))\n",
    "            y = np.vstack((y, y_temp[idxs,:]))\n",
    "            A = np.vstack((A, A_temp[idxs,:]))\n",
    "        \n",
    "        # Cleanup temporary variables to free memory\n",
    "        del array\n",
    "        del X_temp\n",
    "        del y_temp\n",
    "        del A_temp\n",
    "        \n",
    "    return A, X, y\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e1c4acc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "shape, lat, lon = get_data_shape_lat_lon()\n",
    "d = shape[2]*shape[3]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcb70a27",
   "metadata": {},
   "source": [
    "# Training\n",
    "\n",
    "* We train the anchor OPLS model :\n",
    "  * Train/Test splits are randomly sampled accross models (training on 4 model and testing on the 5th)\n",
    "  * Optimal hyperparameters are estimated by Train-validation split for each random split and are then averaged"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5c4d9586",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Randomly sampling across models to create Train/Test splits\n",
    "#B = 15\n",
    "#models_test = random.choices(models, k=B)\n",
    "models_test = models\n",
    "models_train = [[model for model in models if model != model_test ] for model_test in models_test ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a7e7c867",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Counter({'CanESM5': 1,\n",
       "         'MIROC-ES2L': 1,\n",
       "         'MPI-ESM1-2-LR': 1,\n",
       "         'MIROC6': 1,\n",
       "         'CESM2': 1})"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Counting occurence of each test model\n",
    "occurence_models_test = Counter(models_test)\n",
    "occurence_models_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "49d59491",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Trianing parameters\n",
    "var = 'tos' # Selecting the training variable: {tas, pr, psl}\n",
    "\n",
    "N = 1500\n",
    "n_components = np.arange(30, 60, 10)\n",
    "regularisations = [10, 100]\n",
    "coarse = 6\n",
    "gammas = [5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0b970a01",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Weights does not exists!\n"
     ]
    }
   ],
   "source": [
    "# Loading or creating weights\n",
    "try :\n",
    "    directory = '../weights/'\n",
    "    file = 'Anchor_OPLS_tos_weights_stability_n{}_coarse{}.pkl'.format(N, coarse)\n",
    "    with open(directory + file, 'rb') as f:\n",
    "        weights = pickle.load(f)\n",
    "except:\n",
    "    print('Weights does not exists!')\n",
    "    weights = {model:{gamma : {'weights':None, 'n_samples': N*occurence_models_test[model]} for gamma in gammas} for model in models}\n",
    "    \n",
    "for model in models : \n",
    "    for gamma in gammas :\n",
    "        weights[model][gamma]['weights'] = None\n",
    "        weights[model][gamma]['n_samples'] = N*occurence_models_test[model]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7e509055",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'CanESM5': {5: {'weights': None, 'n_samples': 1500}}, 'MIROC-ES2L': {5: {'weights': None, 'n_samples': 1500}}, 'MPI-ESM1-2-LR': {5: {'weights': None, 'n_samples': 1500}}, 'MIROC6': {5: {'weights': None, 'n_samples': 1500}}, 'CESM2': {5: {'weights': None, 'n_samples': 1500}}}\n"
     ]
    }
   ],
   "source": [
    "print(weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2f487611",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/5 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting data\n",
      "Cp covarainces\n",
      "EVD\n",
      "Predictions\n",
      "EVD\n",
      "Predictions\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|██        | 1/5 [01:22<05:29, 82.42s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.26512349179076694 50 10 5\n",
      "Extracting data\n",
      "Cp covarainces\n",
      "EVD\n",
      "Predictions\n",
      "EVD\n",
      "Predictions\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 40%|████      | 2/5 [02:32<03:44, 74.94s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.33153464398672117 50 10 5\n",
      "Extracting data\n",
      "Cp covarainces\n",
      "EVD\n",
      "Predictions\n",
      "EVD\n",
      "Predictions\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 60%|██████    | 3/5 [03:45<02:28, 74.25s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.3280555837082756 50 10 5\n",
      "Extracting data\n",
      "Cp covarainces\n",
      "EVD\n",
      "Predictions\n",
      "EVD\n",
      "Predictions\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 80%|████████  | 4/5 [05:20<01:22, 82.57s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.34163930932516845 50 10 5\n",
      "Extracting data\n",
      "Cp covarainces\n",
      "EVD\n",
      "Predictions\n",
      "EVD\n",
      "Predictions\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5/5 [06:44<00:00, 80.90s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.32237149327482423 50 10 5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Loop through each model in the test set\n",
    "for i in tqdm(range(len(models_test))):\n",
    "    m_train, m_test = models_train[i], models_test[i]\n",
    "    \n",
    "    # Load and preprocess data\n",
    "    print('Extracting data')\n",
    "    A_train, X_train, Y_train = load_data_models(m_train, var=var, n_sample=N, coarse=coarse)\n",
    "    AOP = AnchorOptimalProjection(gamma=gammas)\n",
    "    X_trains, Y_trains = AOP.fit_transform(A_train, X_train, Y_train)\n",
    "    del A_train, X_train, Y_train\n",
    "    \n",
    "    # Loop through each gamma value\n",
    "    for gamma in gammas:\n",
    "        score, ncp_opt, reg_opt = -np.Inf, None, None\n",
    "        \n",
    "        print('Cp covarainces')\n",
    "        # Split data into training and validation sets\n",
    "        X_train, X_val, Y_train, Y_val = train_test_split(X_trains[gamma], Y_trains[gamma])\n",
    "        C_XY = X_train.T @ Y_train \n",
    "        C_XX = X_train.T @ X_train\n",
    "        del X_train, Y_train\n",
    "        \n",
    "        # Regularization\n",
    "        for reg in regularisations:\n",
    "            print('EVD')\n",
    "            C_XX_inv = np.linalg.inv(C_XX + reg * np.identity(C_XX.shape[0]))\n",
    "            C = C_XY.T @ C_XX_inv @ C_XY\n",
    "            u, v = scipy.sparse.linalg.eigs(C, k=np.max(n_components))\n",
    "            u, v = u.real, v.real\n",
    "            idx = np.argsort(u)[::-1]\n",
    "            eig = u[idx]\n",
    "            V = v[:, idx]\n",
    "            U = C_XX_inv @ C_XY @ V\n",
    "                \n",
    "            # Try different numbers of components\n",
    "            print('Predictions')\n",
    "            for n_cp in n_components[::-1]:\n",
    "                Y_pred = X_val @ U[:,:n_cp] @ V.T[:n_cp, :]\n",
    "                score_temp = r2_score(Y_val, Y_pred)\n",
    "                \n",
    "                # Update if the score is better\n",
    "                if score_temp > score:\n",
    "                    score = score_temp\n",
    "                    ncp_opt = n_cp\n",
    "                    reg_opt = reg\n",
    "        \n",
    "        del X_val, Y_val\n",
    "        print(score, ncp_opt, reg_opt, gamma)\n",
    "        \n",
    "        # Save the weights\n",
    "        if weights[m_test][gamma]['weights'] is None:\n",
    "            weights[m_test][gamma]['weights'] = U[:,:ncp_opt] @ V.T[:ncp_opt, :]\n",
    "        else:\n",
    "            weights[m_test][gamma]['weights'] = (weights[m_test][gamma]['n_samples'] * weights[m_test][gamma]['weights'] + N * occurence_models_test[m_test] * U[:,:ncp_opt] @ V.T[:ncp_opt, :]) / (N * occurence_models_test[m_test] + weights[m_test][gamma]['n_samples'])\n",
    "            weights[m_test][gamma]['n_samples'] += N * occurence_models_test[m_test]\n",
    "\n",
    "# Save the weights to a file\n",
    "file_path = f'../weights/Anchor_OPLS_{var}_weights_stability_n{N}_coarse{coarse}.pkl'\n",
    "with open(file_path, 'wb') as f:\n",
    "    pickle.dump(weights, f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c7e12e3d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'../weights/Anchor_OPLS_tos_weights_stability_n1500_coarse6.pkl'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "file_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "04e0dcc1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(288, 10368)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weights[m_test][gamma]['weights'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7765098",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "forcesmip",
   "language": "python",
   "name": "forcesmip"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
