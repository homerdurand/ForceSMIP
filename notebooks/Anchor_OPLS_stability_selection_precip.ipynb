{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6271f056",
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "\n",
    "import xarray as xr\n",
    "import netCDF4 as nc\n",
    "import numpy as np\n",
    "\n",
    "import os\n",
    "\n",
    "import datetime as dt\n",
    "import pickle\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import r2_score\n",
    "import random\n",
    "from tqdm import tqdm\n",
    "from collections import Counter\n",
    "from AnchorOptimalProjector import *\n",
    "import scipy\n",
    "import h5py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91bc34d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "models = ['CanESM5', 'MIROC-ES2L', 'MPI-ESM1-2-LR', 'MIROC6', 'CESM2']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "000cc424",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import h5py\n",
    "import random\n",
    "import numpy as np\n",
    "import xarray as xr\n",
    "\n",
    "def load_model_data(model, var, path='../data/', pkl=True):\n",
    "    \"\"\"\n",
    "    Load model data from either a NetCDF (.nc) or a HDF5 (.h5) file.\n",
    "    \n",
    "    Args:\n",
    "        model (str): Model name.\n",
    "        var (str): Variable name.\n",
    "        path (str, optional): Path to the data directory. Default is '../data/'.\n",
    "        pkl (bool, optional): If True, loads data from a .h5 file; otherwise, from a .nc file. Default is True.\n",
    "        \n",
    "    Returns:\n",
    "        numpy.ndarray: Array containing the loaded model data.\n",
    "    \"\"\"\n",
    "    # Loading data file\n",
    "    file_path = os.path.join(path, '{}_{}.nc'.format(model, var))\n",
    "    ds = xr.open_dataset(file_path)\n",
    "    # Getting variable data\n",
    "    array = ds[var].values\n",
    "    # Close the dataset\n",
    "    ds.close()\n",
    "    return array\n",
    "\n",
    "def get_data_shape_lat_lon(model='CanESM5', var='pr', path='../data/'):\n",
    "    \"\"\"\n",
    "    Get the shape and latitude/longitude coordinates of the data from a NetCDF file.\n",
    "    \n",
    "    Args:\n",
    "        model (str, optional): Model name. Default is 'CanESM5'.\n",
    "        var (str, optional): Variable name. Default is 'pr'.\n",
    "        path (str, optional): Path to the data directory. Default is '../data/'.\n",
    "        \n",
    "    Returns:\n",
    "        tuple: Shape of the data array, latitude coordinate, and longitude coordinate.\n",
    "    \"\"\"\n",
    "    file_path = os.path.join(path, '{}_{}.nc'.format(model, var))\n",
    "    ds = xr.open_dataset(file_path)\n",
    "    # Getting variable data shape\n",
    "    shape = ds[var].values.shape\n",
    "    # Close the dataset\n",
    "    ds.close()\n",
    "    return shape, ds['lat'], ds['lon']\n",
    "\n",
    "def load_data_models(models, var='pr', n_sample=10, path='../data/', coarse=12, window_size=10, pkl=False):\n",
    "    \"\"\"\n",
    "    Load data from multiple models, preprocess it, and return the processed data for further analysis.\n",
    "    \n",
    "    Args:\n",
    "        models (list): List of model names.\n",
    "        var (str, optional): Variable name. Default is 'pr'.\n",
    "        n_sample (int, optional): Number of samples to select randomly from each model's data. Default is 10.\n",
    "        path (str, optional): Path to the data directory. Default is '../data/'.\n",
    "        coarse (int, optional): Coarsening factor for spatial aggregation. Default is 12.\n",
    "        window_size (int, optional): Size of the moving average window. Default is 10.\n",
    "        pkl (bool, optional): If True, loads data from a .h5 file; otherwise, from a .nc file. Default is False.\n",
    "        \n",
    "    Returns:\n",
    "        tuple: Arrays containing the regional internal variability, input data, and target data.\n",
    "    \"\"\"\n",
    "    X, y, A = None, None, None\n",
    "    for model in models:\n",
    "        # Load model data\n",
    "        array = load_model_data(model, var=var, path=path, pkl=pkl)\n",
    "        \n",
    "        shape = array.shape\n",
    "        \n",
    "        # Randomly select n_sample indices\n",
    "        idxs = random.sample(range(shape[0]*shape[1]), n_sample)\n",
    "        \n",
    "        # Reshape data for spatial aggregation\n",
    "        regional = array.reshape(shape[0], shape[1], shape[2]//coarse, coarse, shape[3]//coarse, coarse)\n",
    "        \n",
    "        # Spatially aggregate data\n",
    "        X_temp = regional.mean(axis=(3, 5))\n",
    "        rmt = X_temp.mean(axis=0)\n",
    "        \n",
    "        # Calculate regional internal variability\n",
    "        riv = rmt - X_temp\n",
    "        \n",
    "        # Create a 1D convolution kernel for the moving average\n",
    "        kernel = np.ones(window_size) / window_size\n",
    "        \n",
    "        # Smooth the regional internal variability using a moving average\n",
    "        A_temp = np.apply_along_axis(lambda x: np.convolve(x, kernel, mode='same'), axis=1, arr=riv)\n",
    "        \n",
    "        # Reshape data for further processing\n",
    "        X_temp = X_temp.reshape(shape[0]*shape[1], shape[2]*shape[3]//(coarse**2))\n",
    "        y_temp = np.tile(array.mean(axis=0), (shape[0], 1, 1)).reshape(shape[0]*shape[1], shape[2]*shape[3])\n",
    "        A_temp = A_temp.reshape(shape[0]*shape[1], shape[2]*shape[3]//(coarse**2))\n",
    "\n",
    "        # Concatenate data from current model with existing data\n",
    "        if X is None:\n",
    "            X = X_temp[idxs,:]\n",
    "            y = y_temp[idxs,:]\n",
    "            A = A_temp[idxs, :] \n",
    "        else:\n",
    "            X = np.vstack((X, X_temp[idxs,:]))\n",
    "            y = np.vstack((y, y_temp[idxs,:]))\n",
    "            A = np.vstack((A, A_temp[idxs,:]))\n",
    "        \n",
    "        # Cleanup temporary variables to free memory\n",
    "        del array\n",
    "        del X_temp\n",
    "        del y_temp\n",
    "        del A_temp\n",
    "        \n",
    "    return A, X, y\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1c4acc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "shape, lat, lon = get_data_shape_lat_lon()\n",
    "d = shape[2]*shape[3]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcb70a27",
   "metadata": {},
   "source": [
    "# Training\n",
    "\n",
    "* We train the anchor OPLS model :\n",
    "  * Train/Test splits are randomly sampled accross models (training on 4 model and testing on the 5th)\n",
    "  * Optimal hyperparameters are estimated by Train-validation split for each random split and are then averaged"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c4d9586",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Randomly sampling across models to create Train/Test splits\n",
    "#B = 15\n",
    "#models_test = random.choices(models, k=B)\n",
    "models_test = models\n",
    "models_train = [[model for model in models if model != model_test ] for model_test in models_test ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7e7c867",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Counting occurence of each test model\n",
    "occurence_models_test = Counter(models_test)\n",
    "occurence_models_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49d59491",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Trianing parameters\n",
    "var = 'psl' # Selecting the training variable: {tas, pr, psl}\n",
    "\n",
    "N = 100\n",
    "n_components = np.arange(1, 21, 1)\n",
    "regularisations = [10, 100]\n",
    "coarse = 24\n",
    "gammas = [2, 5, 10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b970a01",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading or creating weights\n",
    "try :\n",
    "    directory = '../weights/'\n",
    "    file = 'Anchor_OPLS_precip_weights_stability_n{}_coarse{}.pkl'.format(N, coarse)\n",
    "    with open(directory + file, 'rb') as f:\n",
    "        weights = pickle.load(f)\n",
    "except:\n",
    "    print('Weights does not exists!')\n",
    "    weights = {model:{gamma : {'weights':None, 'n_samples': N*occurence_models_test[model]} for gamma in gammas} for model in models}\n",
    "    \n",
    "for model in models : \n",
    "    for gamma in gammas :\n",
    "        if gamma not in weights[model].keys():\n",
    "            weights[model][gamma]['weights'] = None\n",
    "            weights[model][gamma]['n_samples'] = N*occurence_models_test[model]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f487611",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Loop through each model in the test set\n",
    "for i in tqdm(range(len(models_test))):\n",
    "    m_train, m_test = models_train[i], models_test[i]\n",
    "    \n",
    "    # Load and preprocess data\n",
    "    A_train, X_train, Y_train = load_data_models(m_train, var=var, n_sample=N, coarse=coarse)\n",
    "    AOP = AnchorOptimalProjection(gamma=gammas)\n",
    "    X_trains, Y_trains = AOP.fit_transform(A_train, X_train, Y_train)\n",
    "    del A_train, X_train, Y_train\n",
    "    \n",
    "    # Loop through each gamma value\n",
    "    for gamma in gammas:\n",
    "        score, ncp_opt, reg_opt = -np.Inf, None, None\n",
    "        \n",
    "        # Split data into training and validation sets\n",
    "        X_train, X_val, Y_train, Y_val = train_test_split(X_trains[gamma], Y_trains[gamma])\n",
    "        C_XY = X_train.T @ Y_train \n",
    "        C_XX = X_train.T @ X_train\n",
    "        del X_train, Y_train\n",
    "        \n",
    "        # Regularization\n",
    "        for reg in regularisations:\n",
    "            C_XX_inv = np.linalg.inv(C_XX + reg * np.identity(C_XX.shape[0]))\n",
    "            C = C_XY.T @ C_XX_inv @ C_XY\n",
    "            u, v = scipy.sparse.linalg.eigs(C, k=np.max(n_components))\n",
    "            u, v = u.real, v.real\n",
    "            idx = np.argsort(u)[::-1]\n",
    "            eig = u[idx]\n",
    "            V = v[:, idx]\n",
    "            U = C_XX_inv @ C_XY @ V\n",
    "                \n",
    "            # Try different numbers of components\n",
    "            for n_cp in n_components[::-1]:\n",
    "                Y_pred = X_val @ U[:,:n_cp] @ V.T[:n_cp, :]\n",
    "                score_temp = r2_score(Y_val, Y_pred)\n",
    "                \n",
    "                # Update if the score is better\n",
    "                if score_temp > score:\n",
    "                    score = score_temp\n",
    "                    ncp_opt = n_cp\n",
    "                    reg_opt = reg\n",
    "        \n",
    "        del X_val, Y_val\n",
    "        print(score, ncp_opt, reg_opt, gamma)\n",
    "        \n",
    "        # Save the weights\n",
    "        if weights[m_test][gamma]['weights'] is None:\n",
    "            weights[m_test][gamma]['weights'] = U[:,:ncp_opt] @ V.T[:ncp_opt, :]\n",
    "        else:\n",
    "            weights[m_test][gamma]['weights'] = (weights[m_test][gamma]['n_samples'] * weights[m_test][gamma]['weights'] + N * occurence_models_test[m_test] * U[:,:ncp_opt] @ V.T[:ncp_opt, :]) / (N * occurence_models_test[m_test] + weights[m_test][gamma]['n_samples'])\n",
    "            weights[m_test][gamma]['n_samples'] += N * occurence_models_test[m_test]\n",
    "\n",
    "# Save the weights to a file\n",
    "file_path = f'../weights/Anchor_OPLS_{var}_weights_stability_n{N}_coarse{coarse}.pkl'\n",
    "with open(file_path, 'wb') as f:\n",
    "    pickle.dump(weights, f)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7ac0534",
   "metadata": {},
   "source": [
    "# Testing "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25accc0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading weights\n",
    "directory = '../weights/'\n",
    "file = 'Anchor_OPLS_precip_weights_stability_n{}_coarse{}.pkl'.format(N, coarse)\n",
    "with open(directory + file, 'rb') as f:\n",
    "    weights = pickle.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58d53652",
   "metadata": {},
   "source": [
    "For each model, we load the weights for which training as not been done on it and evaluate the performance by randomly sampling testing data from this model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6876590",
   "metadata": {},
   "outputs": [],
   "source": [
    "R = 6\n",
    "\n",
    "# Initialize scores_pattern dictionary\n",
    "scores_pattern = {model: {gamma: [] for gamma in weights[model].keys()} for model in models}\n",
    "\n",
    "# Iterate R times\n",
    "for b in tqdm.tqdm(range(R)):\n",
    "    # Iterate through each model\n",
    "    for model in models:\n",
    "        # Iterate through each gamma value for the current model\n",
    "        for gamma in weights[model].keys():\n",
    "            # Load test data\n",
    "            _, X_test, Y_test = load_data_models([model], var='pr', coarse=coarse, n_sample=1000)\n",
    "            \n",
    "            # Load weights\n",
    "            W = weights[model][gamma]['weights']\n",
    "            \n",
    "            # Predict using loaded weights\n",
    "            Y_pred = X_test @ W\n",
    "            \n",
    "            # Compute scores\n",
    "            score_pattern = r2_score(Y_test, Y_pred, multioutput='raw_values')\n",
    "            scores_pattern[model][gamma].append(score_pattern)\n",
    "            \n",
    "            # Clean up variables to free memory\n",
    "            del X_test\n",
    "            del Y_pred\n",
    "            del Y_test\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c248038",
   "metadata": {},
   "outputs": [],
   "source": [
    "gamma=5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8930cb6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.array([np.mean(scores_pattern[model][gamma], axis=1) for model in models]).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d636c6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a boxplot of mean R2 scores for each model\n",
    "plt.boxplot([np.mean(scores_pattern[model][gamma], axis=1) for model in models])\n",
    "\n",
    "# Set xticklabels with model names\n",
    "plt.xticks(range(1, len(models) + 1), [model for model in models], rotation=45)\n",
    "\n",
    "# Set xlabel with models\n",
    "plt.xlabel(r'Models')\n",
    "\n",
    "# Set ylabel with R2 score\n",
    "plt.ylabel(r'$R^2$ Score')\n",
    "\n",
    "# Define directory for saving the plot\n",
    "directory = '../Results'\n",
    "\n",
    "# Save the plot as a PDF file with specified gamma and coarse values in the filename\n",
    "plt.savefig(directory + \"/R2_pr_anchor_OPLS{}_stability_coarse{}_precip.pdf\".format(gamma, coarse), format=\"pdf\", bbox_inches=\"tight\")\n",
    "\n",
    "# Show the plot\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "954fe825",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = 'CanESM5'\n",
    "scores_pattern_maps = np.mean(scores_pattern[model][gamma], axis=0).reshape(len(lat), len(lon))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "666d0f20",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import cartopy.crs as ccrs\n",
    "from cartopy.mpl.gridliner import LONGITUDE_FORMATTER, LATITUDE_FORMATTER\n",
    "from matplotlib.colors import TwoSlopeNorm\n",
    "\n",
    "# Assuming you have defined lat, lon, scores_pattern_maps, gamma, coarse, model\n",
    "\n",
    "# Plotting\n",
    "fig = plt.figure(figsize=(10, 6))\n",
    "ax = plt.axes(projection=ccrs.Robinson())\n",
    "\n",
    "# Set normalization for color scale\n",
    "norm_r2 = TwoSlopeNorm(vmin=-0.5, vcenter=0.0, vmax=0.5)\n",
    "\n",
    "# Plot pcolormesh for scores_pattern_maps\n",
    "contour_r2 = ax.pcolormesh(lon, lat, scores_pattern_maps, transform=ccrs.PlateCarree(), cmap='coolwarm', norm=norm_r2)\n",
    "\n",
    "# Add coastlines\n",
    "ax.coastlines()\n",
    "\n",
    "# Add gridlines with labels\n",
    "gl = ax.gridlines(draw_labels=True)\n",
    "gl.top_labels = gl.right_labels = False\n",
    "gl.xformatter = LONGITUDE_FORMATTER\n",
    "gl.yformatter = LATITUDE_FORMATTER\n",
    "gl.xlabel_style = {'size': 12}  # Longitude font size\n",
    "gl.ylabel_style = {'size': 12}  # Latitude font size\n",
    "\n",
    "# Set title\n",
    "ax.set_title(r'Explained variance of optimal fingerprint (stability)', fontsize=15)\n",
    "\n",
    "# Add colorbar\n",
    "cb = plt.colorbar(contour_r2, ax=ax, label='R2 score differences', orientation='horizontal')\n",
    "cb.ax.tick_params(labelsize=12)\n",
    "cb.set_label(r'$R^2$', fontsize=12) \n",
    "\n",
    "# Define directory for saving the plot\n",
    "directory = '../Results'\n",
    "\n",
    "# Save the plot as a PDF file with specified gamma, coarse, and model values in the filename\n",
    "plt.savefig(directory + \"/maps_R2_anchor{}_stability_coarse_{}_{}_precip.pdf\".format(gamma, coarse, model), format=\"pdf\", bbox_inches=\"tight\")\n",
    "\n",
    "# Show the plot\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "forcesmip",
   "language": "python",
   "name": "forcesmip"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
